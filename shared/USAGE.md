# üìö Gu√≠a de Uso - DataOrchestrator y EmbeddingCache

Esta gu√≠a explica c√≥mo usar los componentes de persistencia en las aplicaciones de Embedding Insights Suite.

---

## üìä DataOrchestrator

Gestor centralizado para todas las operaciones de datos en DuckDB.

### Inicializaci√≥n

```python
from data_orchestrator import DataOrchestrator

# Opci√≥n 1: Desde path directo
orchestrator = DataOrchestrator("workspace/projects/mi-cliente/database.duckdb")

# Opci√≥n 2: Desde configuraci√≥n de proyecto
from data_orchestrator import get_data_orchestrator

project_config = st.session_state.get("project_config")
orchestrator = get_data_orchestrator(project_config)
```

### Gesti√≥n de URLs

```python
# Guardar URLs con metadatos
urls_data = [
    {
        "url": "https://ejemplo.com/page1",
        "title": "T√≠tulo de la p√°gina",
        "content": "Contenido completo...",
        "meta_description": "Descripci√≥n meta",
        "word_count": 500
    },
    {
        "url": "https://ejemplo.com/page2",
        "title": "Otra p√°gina",
        "content": "M√°s contenido...",
        "word_count": 300
    }
]

count = orchestrator.save_urls(urls_data)
print(f"Guardadas {count} URLs")

# Obtener URLs
df = orchestrator.get_urls(limit=100)
print(df.head())

# Filtrar por estado de embedding
pending_df = orchestrator.get_urls(embedding_status="pending")
```

### Gesti√≥n de Embeddings

```python
import numpy as np

# Guardar un embedding
url = "https://ejemplo.com/page1"
embedding = np.random.rand(384)  # Vector de 384 dimensiones
model = "paraphrase-multilingual-MiniLM-L12-v2"

orchestrator.save_embeddings(url, embedding, model)

# Obtener embeddings como DataFrame
df = orchestrator.get_embeddings(model)
print(df[['url', 'dimension', 'created_at']])

# Obtener como matriz numpy
urls, embeddings_matrix = orchestrator.get_embedding_vectors(model)
print(f"URLs: {len(urls)}")
print(f"Shape: {embeddings_matrix.shape}")

# Filtrar URLs espec√≠ficas
specific_urls = ["https://ejemplo.com/page1", "https://ejemplo.com/page2"]
urls, embeddings = orchestrator.get_embedding_vectors(model, urls=specific_urls)
```

### Datos de Google Search Console

```python
import pandas as pd
from datetime import date

# Preparar datos
gsc_data = pd.DataFrame({
    "keyword": ["ejemplo keyword", "otra keyword"],
    "url": ["https://ejemplo.com/page1", "https://ejemplo.com/page2"],
    "position": [3.5, 8.2],
    "impressions": [1000, 500],
    "clicks": [100, 25],
    "ctr": [0.10, 0.05],
    "date": [date.today(), date.today()]
})

# Guardar (reemplaza datos existentes por defecto)
count = orchestrator.save_gsc_data(gsc_data, replace=True)

# Obtener datos con filtros
df = orchestrator.get_gsc_data(
    start_date="2025-01-01",
    end_date="2025-12-31",
    keywords=["ejemplo keyword"]
)
```

### Familias de Keywords

```python
# Guardar familias
families = {
    "Productos": ["comprar producto", "precio producto", "producto barato"],
    "Servicios": ["contratar servicio", "costo servicio", "servicio premium"],
    "Info": ["qu√© es", "c√≥mo funciona", "tutorial"]
}

count = orchestrator.save_keyword_families(families)

# Obtener familias
families_dict = orchestrator.get_keyword_families()
print(families_dict)
```

### Relaciones Sem√°nticas

```python
# Guardar relaciones para linking interno
relations = [
    {
        "source_url": "https://ejemplo.com/page1",
        "target_url": "https://ejemplo.com/page2",
        "similarity_score": 0.85,
        "relation_type": "semantic",
        "anchor_suggestion": "m√°s informaci√≥n sobre..."
    },
    {
        "source_url": "https://ejemplo.com/page1",
        "target_url": "https://ejemplo.com/page3",
        "similarity_score": 0.72,
        "relation_type": "semantic",
        "anchor_suggestion": "ver tambi√©n"
    }
]

count = orchestrator.save_semantic_relations(relations, replace=True)

# Obtener relaciones de una URL
df = orchestrator.get_semantic_relations(
    source_url="https://ejemplo.com/page1",
    min_score=0.7
)
print(df[['target_url', 'similarity_score', 'anchor_suggestion']])
```

### Entidades (Knowledge Graph)

```python
# Guardar entidades extra√≠das
entities = [
    {
        "url": "https://ejemplo.com/page1",
        "entity_text": "Madrid",
        "entity_type": "LOC",
        "frequency": 5,
        "canonical_form": "madrid"
    },
    {
        "url": "https://ejemplo.com/page1",
        "entity_text": "Google",
        "entity_type": "ORG",
        "frequency": 3,
        "canonical_form": "google"
    }
]

count = orchestrator.save_entities(entities, replace=False)

# Obtener entidades
df = orchestrator.get_entities(
    url="https://ejemplo.com/page1",
    entity_type="LOC",
    min_frequency=2
)
```

### Clusters

```python
# Guardar resultados de clustering
clusters_df = pd.DataFrame({
    "url": ["url1", "url2", "url3"],
    "cluster_id": [0, 0, 1],
    "cluster_label": ["Grupo A", "Grupo A", "Grupo B"],
    "distance_to_centroid": [0.1, 0.15, 0.08]
})

count = orchestrator.save_clusters(
    clusters_df,
    model="paraphrase-multilingual-MiniLM-L12-v2",
    replace=True
)

# Obtener clusters
df = orchestrator.get_clusters(model="paraphrase-multilingual-MiniLM-L12-v2")
```

### An√°lisis de FAQs

```python
# Guardar an√°lisis de FAQs
faqs = [
    {
        "question": "¬øQu√© es el SEO?",
        "answer": "El SEO es...",
        "url": "https://ejemplo.com/seo",
        "similarity_score": 0.92,
        "keywords": ["seo", "optimizaci√≥n", "b√∫squeda"]
    }
]

count = orchestrator.save_faq_analysis(faqs, replace=True)

# Obtener FAQs relevantes
df = orchestrator.get_faq_analysis(min_score=0.8)
```

### Estad√≠sticas

```python
# Obtener estad√≠sticas generales
stats = orchestrator.get_stats()
print(stats)
# {'urls': 10, 'embeddings': 10, 'gsc_positions': 500,
#  'keyword_families': 3, 'semantic_relations': 25, ...}
```

---

## üß† EmbeddingCache

Sistema h√≠brido DuckDB + FAISS para cach√© de embeddings con b√∫squeda r√°pida.

### Inicializaci√≥n

```python
from embedding_cache import EmbeddingCache

# Crear cach√© para un proyecto
cache = EmbeddingCache(
    project_path="workspace/projects/mi-cliente",
    model_name="paraphrase-multilingual-MiniLM-L12-v2",
    use_faiss=True  # Usar FAISS si est√° disponible
)
```

### A√±adir Embeddings

```python
import numpy as np

# A√±adir un embedding
url = "https://ejemplo.com/page1"
embedding = np.random.rand(384)

cache.add_embedding(url, embedding, rebuild_index=False)

# A√±adir m√∫ltiples embeddings (batch)
urls = ["url1.com", "url2.com", "url3.com"]
embeddings = np.random.rand(3, 384)

count = cache.add_embeddings_batch(urls, embeddings)
print(f"A√±adidos {count} embeddings")
# El √≠ndice FAISS se reconstruye autom√°ticamente
```

### Obtener Embeddings

```python
# Obtener un embedding espec√≠fico
embedding = cache.get_embedding("https://ejemplo.com/page1")
if embedding is not None:
    print(f"Dimensi√≥n: {len(embedding)}")

# Verificar si existe
exists = cache.has_embedding("https://ejemplo.com/page1")

# Obtener todos los embeddings
urls, embeddings_matrix = cache.get_all_embeddings()
print(f"Total URLs: {len(urls)}")
print(f"Shape: {embeddings_matrix.shape}")
```

### B√∫squeda de Similitud

```python
# Buscar los 10 m√°s similares
query_embedding = np.random.rand(384)

results = cache.search_similar(
    query_embedding,
    top_k=10,
    exclude_urls=["https://ejemplo.com/self"]  # Excluir URLs espec√≠ficas
)

for result in results:
    print(f"URL: {result['url']}")
    print(f"Similitud: {result['similarity']:.3f}")
    print(f"Distancia: {result['distance']:.3f}")
    print("---")

# Si FAISS est√° disponible, la b√∫squeda es 100-1000x m√°s r√°pida
# Si no, usa b√∫squeda lineal con numpy (fallback autom√°tico)
```

### Helper: Get or Compute

```python
from embedding_cache import get_or_compute_embedding

def compute_embedding_fn(url, model):
    # Tu l√≥gica para computar embedding
    # Por ejemplo, con sentence-transformers
    from sentence_transformers import SentenceTransformer
    model_obj = SentenceTransformer(model)
    # Extraer texto de la URL y computar
    text = extract_text(url)
    return model_obj.encode(text)

# Obtiene del cach√© o computa si no existe
embedding = get_or_compute_embedding(
    cache,
    url="https://ejemplo.com/page1",
    compute_fn=compute_embedding_fn,
    model="paraphrase-multilingual-MiniLM-L12-v2"
)
```

### Sincronizaci√≥n y Mantenimiento

```python
# Sincronizar √≠ndice FAISS con DuckDB
cache.sync_from_db()

# Obtener estad√≠sticas del cach√©
stats = cache.get_cache_stats()
print(stats)
# {
#   'model': 'paraphrase-multilingual-MiniLM-L12-v2',
#   'count': 100,
#   'dimension': 384,
#   'faiss_enabled': True,
#   'faiss_index_exists': True,
#   'faiss_size_mb': 0.5
# }

# Limpiar cach√© (solo FAISS, no DuckDB)
cache.clear_cache(confirm=True)
```

---

## üîó Integraci√≥n en Streamlit

### Ejemplo completo con project selector

```python
import streamlit as st
from data_orchestrator import get_data_orchestrator
from embedding_cache import EmbeddingCache

# Obtener proyecto actual
project_config = st.session_state.get("project_config")

if not project_config:
    st.warning("Selecciona un proyecto en el sidebar")
    st.stop()

# Inicializar componentes
orchestrator = get_data_orchestrator(project_config)
cache = EmbeddingCache(
    project_path=project_config["path"],
    model_name="paraphrase-multilingual-MiniLM-L12-v2"
)

# Usar en tu l√≥gica
if st.button("Procesar URLs"):
    urls = ["url1.com", "url2.com"]

    # Guardar URLs
    orchestrator.save_urls([
        {"url": url, "title": f"T√≠tulo {url}"}
        for url in urls
    ])

    # Computar y guardar embeddings
    embeddings = compute_embeddings(urls)
    cache.add_embeddings_batch(urls, embeddings)

    st.success(f"Procesadas {len(urls)} URLs")

# Mostrar estad√≠sticas
col1, col2 = st.columns(2)
with col1:
    db_stats = orchestrator.get_stats()
    st.metric("URLs en DB", db_stats['urls'])

with col2:
    cache_stats = cache.get_cache_stats()
    st.metric("Embeddings", cache_stats['count'])
```

---

## üí° Mejores Pr√°cticas

### DataOrchestrator

1. **Usa get_data_orchestrator()** en lugar de instanciar directamente
2. **Guarda en batch** cuando sea posible para mejor rendimiento
3. **Usa filtros** en get_* para reducir memoria
4. **Cierra conexiones** (se hace autom√°ticamente, pero ten en cuenta)

### EmbeddingCache

1. **Usa batch inserts** para m√∫ltiples embeddings
2. **Habilita FAISS** para datasets grandes (>1000 embeddings)
3. **No rebuilds frecuentes** - solo despu√©s de batch inserts
4. **Sincroniza despu√©s de cambios** externos en DuckDB
5. **Normaliza embeddings** antes de b√∫squeda si es necesario

### General

1. **Verifica proyecto activo** antes de usar orchestrator/cache
2. **Maneja errores** - ambas clases pueden lanzar excepciones
3. **Monitorea tama√±o** de DB con get_stats()
4. **Backups peri√≥dicos** de database.duckdb

---

## üêõ Troubleshooting

**Error: DuckDB no est√° instalado**
```bash
pip install duckdb
```

**Error: FAISS no est√° disponible**
```bash
# FAISS es opcional, la b√∫squeda funcionar√° sin √©l (m√°s lento)
pip install faiss-cpu  # O faiss-gpu si tienes CUDA
```

**Error: Embedding dimension mismatch**
- Verifica que todos los embeddings tengan la misma dimensi√≥n
- No mezcles modelos diferentes en el mismo cach√©

**Performance lento en b√∫squeda**
- Activa FAISS: `use_faiss=True`
- Verifica que el √≠ndice FAISS se haya construido: `cache.sync_from_db()`

**Database locked**
- DuckDB no permite m√∫ltiples escritores
- Cierra otras conexiones antes de escribir
- Usa `read_only=True` para lecturas paralelas

---

## üìñ Documentaci√≥n Adicional

- [ROADMAP.md](../ROADMAP.md) - Plan de desarrollo completo
- [shared/db_schema.py](db_schema.py) - Definici√≥n del schema DuckDB
- [shared/project_manager.py](project_manager.py) - Gesti√≥n de proyectos

**√öltima actualizaci√≥n:** 2025-12-30
**Versi√≥n:** 1.0.0
